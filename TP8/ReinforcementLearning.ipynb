{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 200/200 [01:00<00:00,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Q:\n",
      "[[-472.26996053 -240.89845611 -112.27439679 -465.33821153]\n",
      " [-466.10848456 -350.80603789 -121.07732811 -176.12307014]\n",
      " [ -92.7033605   -29.17787555 -250.21429019 -453.27947237]\n",
      " [-472.26996053 -341.45106572 -112.2510292  -464.40753859]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "environment = np.array([\n",
    "    [0, 0, 0, 1],\n",
    "    [0, 1, 0, 1],\n",
    "    [0, 1, 0, 2],\n",
    "    [0, 0, 0, 1]\n",
    "])\n",
    "\n",
    "gamma = 0.8\n",
    "epsilon = 0.1\n",
    "learning_rate = 0.9\n",
    "\n",
    "q_table = np.zeros((4, 4))\n",
    "\n",
    "def choose_action(state):\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        return np.random.choice([0, 1, 2, 3])  # Actions: haut, bas, gauche, droite\n",
    "    else:\n",
    "        return np.argmax(q_table[state])\n",
    "\n",
    "def update_q_table(state, action, reward, next_state):\n",
    "    q_predict = q_table[state, action]\n",
    "    q_target = reward + gamma * np.max(q_table[next_state])\n",
    "    q_table[state, action] += learning_rate * (q_target - q_predict)\n",
    "\n",
    "for episode in tqdm(range(200), desc='Training'):\n",
    "    state = (0, 0)\n",
    "    while True:\n",
    "        action = choose_action(state)\n",
    "        next_state = (state[0] + [-1, 1, 0, 0][action], state[1] + [0, 0, -1, 1][action])\n",
    "        next_state = (max(0, min(next_state[0], environment.shape[0]-1)), max(0, min(next_state[1], environment.shape[1]-1)))\n",
    "        reward = -1 if environment[next_state] == 0 else 10 if environment[next_state] == 2 else -100\n",
    "        update_q_table(state, action, reward, next_state)\n",
    "        state = next_state\n",
    "        if environment[state] == 2:\n",
    "            break\n",
    "\n",
    "print(\"Table Q:\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions pour chaque état final:\n",
      "État (2, 3):\n",
      "   Aller en haut: Q-value = -453.2794723695845\n",
      "   Aller en bas: Q-value = -453.2794723695845\n",
      "   Aller à gauche: Q-value = -453.2794723695845\n",
      "   Aller à droite: Q-value = -453.2794723695845\n"
     ]
    }
   ],
   "source": [
    "# Affichage des actions pour chaque état final\n",
    "print(\"Actions pour chaque état final:\")\n",
    "for i in range(environment.shape[0]):\n",
    "    for j in range(environment.shape[1]):\n",
    "        if environment[i, j] == 2:\n",
    "            print(f\"État ({i}, {j}):\")\n",
    "            for action, action_name in enumerate([\"Aller en haut\", \"Aller en bas\", \"Aller à gauche\", \"Aller à droite\"]):\n",
    "                print(f\"   {action_name}: Q-value = {q_table[i, j]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [02:21<00:00,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Q:\n",
      "[[-433.66617114 -122.59617277   -5.05893407 -417.0832497 ]\n",
      " [-328.17112923 -340.65901051   -7.42733816  -23.62195854]\n",
      " [  -5.04714726 -207.18611367 -157.68902775 -417.08315029]\n",
      " [-433.6665852  -303.32303942   -5.05910201 -230.45476975]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "environment = np.array([\n",
    "    [0, 0, 0, 1],\n",
    "    [0, 1, 0, 1],\n",
    "    [0, 1, 0, 2],\n",
    "    [0, 0, 0, 1]\n",
    "])\n",
    "\n",
    "gamma = 0.8\n",
    "epsilon = 0.1\n",
    "learning_rate = 0.9\n",
    "\n",
    "q_table = np.zeros((4, 4))\n",
    "\n",
    "def choose_action(state):\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        return np.random.choice([0, 1, 2, 3])  # Actions: haut, bas, gauche, droite\n",
    "    else:\n",
    "        return np.argmax(q_table[state])\n",
    "\n",
    "def update_q_table(state, action, reward, next_state):\n",
    "    q_predict = q_table[state, action]\n",
    "    q_target = reward + gamma * np.max(q_table[next_state])\n",
    "    q_table[state, action] += learning_rate * (q_target - q_predict)\n",
    "\n",
    "for episode in tqdm(range(500), desc='Training'):\n",
    "    state = (0, 0)\n",
    "    while True:\n",
    "        action = choose_action(state)\n",
    "        next_state = (state[0] + [-1, 1, 0, 0][action], state[1] + [0, 0, -1, 1][action])\n",
    "        next_state = (max(0, min(next_state[0], environment.shape[0]-1)), max(0, min(next_state[1], environment.shape[1]-1)))\n",
    "        reward = -1 if environment[next_state] == 0 else 10 if environment[next_state] == 2 else -100\n",
    "        update_q_table(state, action, reward, next_state)\n",
    "        state = next_state\n",
    "        if environment[state] == 2:\n",
    "            break\n",
    "\n",
    "print(\"Table Q:\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions pour chaque état final:\n",
      "État (2, 3):\n",
      "   Aller en haut: Q-value = -417.0831502925515\n",
      "   Aller en bas: Q-value = -417.0831502925515\n",
      "   Aller à gauche: Q-value = -417.0831502925515\n",
      "   Aller à droite: Q-value = -417.0831502925515\n"
     ]
    }
   ],
   "source": [
    "# Affichage des actions pour chaque état final\n",
    "print(\"Actions pour chaque état final:\")\n",
    "for i in range(environment.shape[0]):\n",
    "    for j in range(environment.shape[1]):\n",
    "        if environment[i, j] == 2:\n",
    "            print(f\"État ({i}, {j}):\")\n",
    "            for action, action_name in enumerate([\"Aller en haut\", \"Aller en bas\", \"Aller à gauche\", \"Aller à droite\"]):\n",
    "                print(f\"   {action_name}: Q-value = {q_table[i, j]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [04:49<00:00,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Q:\n",
      "[[-458.18759359 -104.98705979   -5.58425003 -447.84896986]\n",
      " [-439.13776325 -335.57453765   -7.7454857    -6.35618543]\n",
      " [  -5.46740002   -7.19640277  -18.51728988 -420.32614464]\n",
      " [-458.18759359 -304.15716748   -5.58423511 -447.61391517]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "environment = np.array([\n",
    "    [0, 0, 0, 1],\n",
    "    [0, 1, 0, 1],\n",
    "    [0, 1, 0, 2],\n",
    "    [0, 0, 0, 1]\n",
    "])\n",
    "\n",
    "gamma = 0.8\n",
    "epsilon = 0.1\n",
    "learning_rate = 0.9\n",
    "\n",
    "q_table = np.zeros((4, 4))\n",
    "\n",
    "def choose_action(state):\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        return np.random.choice([0, 1, 2, 3])  # Actions: haut, bas, gauche, droite\n",
    "    else:\n",
    "        return np.argmax(q_table[state])\n",
    "\n",
    "def update_q_table(state, action, reward, next_state):\n",
    "    q_predict = q_table[state, action]\n",
    "    q_target = reward + gamma * np.max(q_table[next_state])\n",
    "    q_table[state, action] += learning_rate * (q_target - q_predict)\n",
    "\n",
    "for episode in tqdm(range(1000), desc='Training'):\n",
    "    state = (0, 0)\n",
    "    while True:\n",
    "        action = choose_action(state)\n",
    "        next_state = (state[0] + [-1, 1, 0, 0][action], state[1] + [0, 0, -1, 1][action])\n",
    "        next_state = (max(0, min(next_state[0], environment.shape[0]-1)), max(0, min(next_state[1], environment.shape[1]-1)))\n",
    "        reward = -1 if environment[next_state] == 0 else 10 if environment[next_state] == 2 else -100\n",
    "        update_q_table(state, action, reward, next_state)\n",
    "        state = next_state\n",
    "        if environment[state] == 2:\n",
    "            break\n",
    "\n",
    "print(\"Table Q:\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions pour chaque état final:\n",
      "État (2, 3):\n",
      "   Aller en haut: Q-value = -420.3261446406493\n",
      "   Aller en bas: Q-value = -420.3261446406493\n",
      "   Aller à gauche: Q-value = -420.3261446406493\n",
      "   Aller à droite: Q-value = -420.3261446406493\n"
     ]
    }
   ],
   "source": [
    "# Affichage des actions pour chaque état final\n",
    "print(\"Actions pour chaque état final:\")\n",
    "for i in range(environment.shape[0]):\n",
    "    for j in range(environment.shape[1]):\n",
    "        if environment[i, j] == 2:\n",
    "            print(f\"État ({i}, {j}):\")\n",
    "            for action, action_name in enumerate([\"Aller en haut\", \"Aller en bas\", \"Aller à gauche\", \"Aller à droite\"]):\n",
    "                print(f\"   {action_name}: Q-value = {q_table[i, j]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
